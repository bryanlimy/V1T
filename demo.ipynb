{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Demo code to run V1T on sensorium+ test set and extract attention rollout maps\n",
    "\n",
    "Please follow the instruction in [README.md](README.md) to set up the conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T12:18:51.095243Z",
     "end_time": "2023-04-21T12:18:52.672945Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import typing as t\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from v1t import data\n",
    "from v1t.utils import utils\n",
    "from v1t.models import Model\n",
    "from v1t.metrics import Metrics\n",
    "from v1t.utils.scheduler import Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "create dummy arg object to mimic argparse and load args from checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T12:18:52.673771Z",
     "end_time": "2023-04-21T12:18:52.677966Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.batch_size = 16\n",
    "        self.output_dir = \"runs/best_v1t_sensorium\"\n",
    "        self.dataset = \"data/sensorium\"\n",
    "\n",
    "\n",
    "args = Args()\n",
    "utils.load_args(args)  # load arguments from checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "load Sensorium dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T12:18:52.678764Z",
     "end_time": "2023-04-21T12:18:52.730673Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = data.get_training_ds(\n",
    "    args,\n",
    "    data_dir=args.dataset,\n",
    "    mouse_ids=args.mouse_ids,\n",
    "    batch_size=args.batch_size,\n",
    "    device=args.device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "initialize model and restore checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T12:18:52.731328Z",
     "end_time": "2023-04-21T12:18:52.816597Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Model(args, ds=train_ds)\n",
    "model.to(args.device)\n",
    "\n",
    "scheduler = Scheduler(args, model=model, save_optimizer=False)\n",
    "_ = scheduler.restore(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T12:18:52.819996Z",
     "end_time": "2023-04-21T12:18:52.821978Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference(\n",
    "    ds: DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    batch_size: int,\n",
    "    device: torch.device = \"cpu\",\n",
    ") -> t.Dict[str, torch.Tensor]:\n",
    "    \"\"\"Inference data in DataLoader ds\n",
    "    Returns:\n",
    "        results: t.Dict[int, t.Dict[str, torch.Tensor]]\n",
    "            - mouse_id\n",
    "                - predictions: torch.Tensor, predictions given images\n",
    "                - targets: torch.Tensor, the ground-truth responses\n",
    "                - trial_ids: torch.Tensor, trial ID of the responses\n",
    "                - image_ids: torch.Tensor, image ID of the responses\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"predictions\": [],\n",
    "        \"targets\": [],\n",
    "        \"trial_ids\": [],\n",
    "        \"image_ids\": [],\n",
    "    }\n",
    "    mouse_id = ds.dataset.mouse_id\n",
    "    model.to(device)\n",
    "    model.train(False)\n",
    "    for batch in tqdm(ds, desc=f\"Mouse {mouse_id}\"):\n",
    "        for micro_batch in data.micro_batching(batch, batch_size=batch_size):\n",
    "            predictions, _, _ = model(\n",
    "                inputs=micro_batch[\"image\"].to(device),\n",
    "                mouse_id=mouse_id,\n",
    "                behaviors=micro_batch[\"behavior\"].to(device),\n",
    "                pupil_centers=micro_batch[\"pupil_center\"].to(device),\n",
    "            )\n",
    "            results[\"predictions\"].append(predictions.cpu())\n",
    "            results[\"targets\"].append(micro_batch[\"response\"])\n",
    "            results[\"image_ids\"].append(micro_batch[\"image_id\"])\n",
    "            results[\"trial_ids\"].append(micro_batch[\"trial_id\"])\n",
    "    results = {\n",
    "        k: torch.cat(v, dim=0) if isinstance(v[0], torch.Tensor) else v\n",
    "        for k, v in results.items()\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-19T13:12:52.989787083Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for mouse_id in [\"A\", \"B\", \"C\", \"D\", \"E\"]:\n",
    "    outputs = inference(\n",
    "        ds=test_ds[mouse_id],\n",
    "        model=model,\n",
    "        batch_size=args.batch_size,\n",
    "        device=args.device,\n",
    "    )\n",
    "    metrics = Metrics(ds=test_ds[mouse_id], results=outputs)\n",
    "\n",
    "    single_trial_correlation = metrics.single_trial_correlation(per_neuron=False)\n",
    "    correlation_to_average = metrics.correlation_to_average(per_neuron=False)\n",
    "    print(\n",
    "        f\"single trial correlation: {single_trial_correlation:.03f}\\n\"\n",
    "        f\"correlation to average: {correlation_to_average:.03f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Extract attention rollout maps for Mouse A\n",
    "\n",
    "See [misc/vit_visualization.py](misc/vit_visualization.py) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T17:05:22.462564Z",
     "start_time": "2023-04-19T17:05:22.454634Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from v1t.utils import tensorboard\n",
    "from v1t.utils.attention_rollout import attention_rollouts, Recorder\n",
    "\n",
    "tensorboard.set_font()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "helper function to overlay attention rollout maps on input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T17:06:35.921827Z",
     "start_time": "2023-04-19T17:06:35.918464Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_attention_maps(\n",
    "    val_results: t.Dict[str, np.ndarray],\n",
    "    test_results: t.Dict[str, np.ndarray],\n",
    "    colormap: str = \"turbo\",\n",
    "    alpha: float = 0.5,\n",
    "):\n",
    "    cmap = matplotlib.colormaps.get_cmap(colormap)\n",
    "    colors = cmap(np.arange(256))[:, :3]\n",
    "    label_fontsize, tick_fontsize = 10, 8\n",
    "    labelpad = 2\n",
    "    figure, axes = plt.subplots(\n",
    "        nrows=2,\n",
    "        ncols=3,\n",
    "        figsize=(8, 4),\n",
    "        gridspec_kw={\"wspace\": 0.05, \"hspace\": -0.25},\n",
    "        dpi=240,\n",
    "    )\n",
    "    for i in range(len(val_results[\"images\"])):\n",
    "        image = val_results[\"images\"][i][0]\n",
    "        heatmap = val_results[\"heatmaps\"][i]\n",
    "        behavior = val_results[\"behaviors\"][i]\n",
    "        pupil_center = val_results[\"pupil_centers\"][i]\n",
    "        heatmap = colors[np.uint8(255.0 * heatmap)] * 255.0\n",
    "        heatmap = alpha * heatmap + (1 - alpha) * image[..., None]\n",
    "        axes[0, i].imshow(heatmap.astype(np.uint8), cmap=colormap)\n",
    "        axes[0, i].set_xticks([])\n",
    "        axes[0, i].set_yticks([])\n",
    "        tensorboard.remove_spines(axis=axes[0, i])\n",
    "        description = (\n",
    "            f\"[{behavior[0]:.01f}, \"  # pupil dilation\n",
    "            f\"{behavior[1]:.01f}, \"  # dilation derivative\n",
    "            f\"({pupil_center[0]:.01f}, {pupil_center[1]:.01f}), \"  # pupil center\n",
    "            f\"{behavior[2]:.01f}]\"  # speed\n",
    "        )\n",
    "        axes[0, i].set_xlabel(description, labelpad=labelpad, fontsize=tick_fontsize)\n",
    "    axes[0, 0].set_ylabel(\n",
    "        \"Validation samples\", labelpad=labelpad, fontsize=tick_fontsize\n",
    "    )\n",
    "    for i in range(len(test_results[\"images\"])):\n",
    "        image = test_results[\"images\"][i][0]\n",
    "        heatmap = test_results[\"heatmaps\"][i]\n",
    "        behavior = test_results[\"behaviors\"][i]\n",
    "        pupil_center = test_results[\"pupil_centers\"][i]\n",
    "        heatmap = colors[np.uint8(255.0 * heatmap)] * 255.0\n",
    "        heatmap = alpha * heatmap + (1 - alpha) * image[..., None]\n",
    "        axes[1, i].imshow(heatmap.astype(np.uint8), cmap=colormap)\n",
    "        axes[1, i].set_xticks([])\n",
    "        axes[1, i].set_yticks([])\n",
    "        tensorboard.remove_spines(axis=axes[1, i])\n",
    "        description = (\n",
    "            f\"[{behavior[0]:.01f}, \"  # pupil dilation\n",
    "            f\"{behavior[1]:.01f}, \"  # dilation derivative\n",
    "            f\"({pupil_center[0]:.01f}, {pupil_center[1]:.01f}), \"  # pupil center\n",
    "            f\"{behavior[2]:.01f}]\"  # speed\n",
    "        )\n",
    "        axes[1, i].set_xlabel(description, labelpad=labelpad, fontsize=tick_fontsize)\n",
    "    axes[-1, 0].set_ylabel(\"Test samples\", labelpad=labelpad, fontsize=tick_fontsize)\n",
    "\n",
    "    # plot colorbar\n",
    "    pos1 = axes[0, -1].get_position()\n",
    "    pos2 = axes[-1, -1].get_position()\n",
    "    width, height = 0.008, (pos1.y1 - pos1.y0) * 0.35\n",
    "    cbar_ax = figure.add_axes(\n",
    "        rect=[\n",
    "            pos1.x1 + 0.01,\n",
    "            ((pos1.y1 - pos2.y0) / 2 + pos2.y0) - (height / 2),\n",
    "            width,\n",
    "            height,\n",
    "        ]\n",
    "    )\n",
    "    figure.colorbar(cm.ScalarMappable(cmap=colormap), cax=cbar_ax, shrink=0.1)\n",
    "    tensorboard.set_yticks(\n",
    "        axis=cbar_ax,\n",
    "        ticks_loc=np.linspace(0, 1, 3),\n",
    "        tick_fontsize=tick_fontsize,\n",
    "    )\n",
    "    tensorboard.set_ticks_params(axis=cbar_ax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "helper function to record outputs from scaled dot product attention and extract attention rollout maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T17:06:36.641670Z",
     "start_time": "2023-04-19T17:06:36.631915Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_attention_maps(\n",
    "    ds: DataLoader, model: Model, num_plots: int, device: torch.device = \"cpu\"\n",
    ") -> t.Dict[str, np.ndarray]:\n",
    "    model.to(device)\n",
    "    model.train(False)\n",
    "\n",
    "    mouse_id = ds.dataset.mouse_id\n",
    "    i_transform_image = ds.dataset.i_transform_image\n",
    "    i_transform_behavior = ds.dataset.i_transform_behavior\n",
    "    i_transform_pupil_center = ds.dataset.i_transform_pupil_center\n",
    "\n",
    "    recorder = Recorder(model.core)\n",
    "    results = {\"images\": [], \"heatmaps\": [], \"pupil_centers\": [], \"behaviors\": []}\n",
    "    count = num_plots\n",
    "    for i, batch in enumerate(ds):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        behaviors = batch[\"behavior\"].to(device)\n",
    "        pupil_centers = batch[\"pupil_center\"].to(device)\n",
    "        images, _ = model.image_cropper(\n",
    "            inputs=images,\n",
    "            mouse_id=mouse_id,\n",
    "            behaviors=behaviors,\n",
    "            pupil_centers=pupil_centers,\n",
    "        )\n",
    "        _, attentions = recorder(\n",
    "            images=images,\n",
    "            behaviors=behaviors,\n",
    "            pupil_centers=pupil_centers,\n",
    "            mouse_id=mouse_id,\n",
    "        )\n",
    "        recorder.clear()\n",
    "\n",
    "        # compute attention rollout maps within the loop to avoid OOM\n",
    "        heatmaps = attention_rollouts(attentions, image_shape=images.shape[-2:])\n",
    "\n",
    "        results[\"images\"].append(i_transform_image(images.cpu()))\n",
    "        results[\"heatmaps\"].append(heatmaps.cpu())\n",
    "        results[\"behaviors\"].append(i_transform_behavior(behaviors.cpu()))\n",
    "        results[\"pupil_centers\"].append(i_transform_pupil_center(pupil_centers.cpu()))\n",
    "\n",
    "        if (count := count - len(images)) <= 0:\n",
    "            break\n",
    "\n",
    "    recorder.eject()\n",
    "    del recorder\n",
    "\n",
    "    return {k: torch.vstack(v).numpy()[:num_plots] for k, v in results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T17:06:59.226955Z",
     "start_time": "2023-04-19T17:06:36.967103Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mouse_id = \"A\"\n",
    "num_plots = 3\n",
    "val_results = extract_attention_maps(\n",
    "    ds=val_ds[mouse_id],\n",
    "    model=model,\n",
    "    num_plots=num_plots,\n",
    "    device=args.device,\n",
    ")\n",
    "test_results = extract_attention_maps(\n",
    "    ds=test_ds[mouse_id],\n",
    "    model=model,\n",
    "    num_plots=num_plots,\n",
    "    device=args.device,\n",
    ")\n",
    "plot_attention_maps(val_results=val_results, test_results=test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Please check [misc/](misc/) for code to generate plots and figures used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
